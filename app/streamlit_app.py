# app/streamlit_app.py
 
import streamlit as st
import pandas as pd
import joblib
import json
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
 
# ===============================
# Load model metadata
# ===============================
metadata_path = "../models/model_metadata.json"
os.makedirs("../models", exist_ok=True)
 
if not os.path.exists(metadata_path):
    metadata = {"current_version": 1, "versions": {}}
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)
else:
    with open(metadata_path, "r") as f:
        metadata = json.load(f)
 
v = metadata.get("current_version", 1)
 
# ===============================
# Load current model + tools
# ===============================
model = joblib.load(f"../models/trained_model_v{v}.pkl")
scaler = joblib.load(f"../models/scaler_v{v}.pkl")
le = joblib.load(f"../models/label_encoder_v{v}.pkl")
FEATURES = joblib.load("../models/feature_list.pkl")
 
# ===============================
# Streamlit UI setup
# ===============================
st.set_page_config(page_title="ü™ê Kepler Exoplanet Classifier", layout="wide")
st.title("ü™ê Kepler Exoplanet Classifier")
 
st.markdown(f"""
### Current Model Version: **v{v}**
This AI model classifies potential **exoplanet detections** as:
- üü¢ **CONFIRMED EXOPLANET** ‚Äî Definitely a planet
- üü° **PLANETARY CANDIDATE** ‚Äî Likely a planet, not yet confirmed
- üî¥ **FALSE POSITIVE** ‚Äî Probably not a planet due to various
""")
 
# ===============================
# Feature Importance
# ===============================
st.subheader("üåü Top Features Influencing Planet Detection")
feature_plot_path = f"../models/feature_importances_v{v}.png"
 
if os.path.exists(feature_plot_path):
    st.image(feature_plot_path, caption=f"Feature Importance (Model v{v})", width=800)
else:
    st.warning("‚ö†Ô∏è Feature importance plot not found. Please train or retrain the model first.")
 
# ===============================
# üìÇ Option 1: Upload file for prediction
# ===============================
st.header("üìÇ Predict Using Uploaded Data")
uploaded_file = st.file_uploader("Upload a CSV file with Kepler features", type=["csv"])
 
if uploaded_file:
    new_data = pd.read_csv(uploaded_file)
    st.write("‚úÖ Uploaded Data Preview:", new_data.head())
 
    try:
        new_data = new_data[FEATURES]
        new_data = new_data.fillna(new_data.median())
        new_data_scaled = scaler.transform(new_data)
        predictions = model.predict(new_data_scaled)
        predicted_labels = le.inverse_transform(predictions)
 
        new_data["Predicted Class"] = predicted_labels
        st.subheader("üßÆ Predictions")
        st.dataframe(new_data.head())
 
        csv = new_data.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è Download Predictions", csv, "exoplanet_predictions.csv", "text/csv")
    except Exception as e:
        st.error(f"‚ùå Error during prediction: {e}")
 
# ===============================
# üî≠ Option 2: Manual Input
# ===============================
st.header("üî≠ Predict Using Manual Input")
 
with st.form("manual_input_form"):
    st.subheader("Observational Parameters")
 
    col1, col2 = st.columns(2)
    with col1:
        koi_period = st.number_input("Orbital Period (days)", 0.0, 500.0, 45.3)
        koi_depth = st.number_input("Transit Depth (ppm)", 0.0, 20000.0, 1200.0)
        koi_prad = st.number_input("Planetary Radius (Earth radii)", 0.0, 20.0, 1.2)
        koi_eqtemp = st.number_input("Equilibrium Temperature (K)", 0.0, 3000.0, 580.0)
    with col2:
        koi_stefftemp = st.number_input("Stellar Temperature (K)", 3000.0, 10000.0, 5500.0)
        koi_srad = st.number_input("Stellar Radius (solar radii)", 0.0, 10.0, 0.9)
        koi_slogg = st.number_input("Stellar Surface Gravity (log10(cm/s¬≤))", 0.0, 6.0, 4.4)
        koi_kepmag = st.number_input("Kepler-band Magnitude", 0.0, 20.0, 13.2)
 
    st.markdown("---")
 
    st.warning("""
     **Note:** Features like `disposition_score`, `Centroid Offset Flag`, `Stellar Eclipse Flag`, `Not Transit-Like Flag`, and `Ephemeris Match Indicates Contamination Flag` are **system-generated by NASA‚Äôs pipeline**.
    Best left unchanged unless you're a space scientist validating data. They are listed under "Advanced Mode" below.
    """)
 
    with st.expander("‚öôÔ∏è Advanced (System Features)"):
        disposition_score = st.slider("Disposition Score (0‚Äì1)", 0.0, 1.0, 0.5)
        ntl_fpflag = st.selectbox("Not Transit-Like Flag", [0, 1], index=0)
        se_fpflag = st.selectbox("Stellar Eclipse Flag", [0, 1], index=0)
        co_fpflag = st.selectbox("Centroid Offset Flag", [0, 1], index=0)
        ec_fpflag = st.selectbox("Ephemeris Contamination Flag", [0, 1], index=0)
 
    submitted = st.form_submit_button("üöÄ Classify Object")
 
if submitted:
    try:
        input_data = pd.DataFrame([[
            disposition_score, ntl_fpflag, se_fpflag, co_fpflag, ec_fpflag,
            koi_period, koi_depth, koi_prad, koi_eqtemp,
            koi_stefftemp, koi_srad, koi_slogg, koi_kepmag
        ]], columns=FEATURES)
 
        input_scaled = scaler.transform(input_data)
        pred = model.predict(input_scaled)
        label = le.inverse_transform(pred)[0]
 
        color_map = {
            "CONFIRMED": "üü¢",
            "CANDIDATE": "üü°",
            "FALSE POSITIVE": "üî¥"
        }
        st.success(f"{color_map.get(label.upper(), '‚ú®')} Predicted Class: **{label}**")
    except Exception as e:
        st.error(f"‚ùå Error in classification: {e}")
 
# ===============================
# üß† Retrain and Version Your Model
# ===============================
st.header("üß† Retrain and Version Your Model")
 
# --- Ensure metadata keys exist
metadata.setdefault("versions", {})
metadata.setdefault("current_version", metadata.get("current_version", 1))
 
# --- Session state: hold candidate artifacts between reruns
for k, v_default in {
    "candidate_model": None,
    "candidate_scaler": None,
    "candidate_le": None,
    "candidate_metrics": None,     # {"accuracy": float}
    "candidate_features": None,    # list[str]
    "candidate_featimp_path": None # path to PNG
}.items():
    if k not in st.session_state:
        st.session_state[k] = v_default
 
uploaded_new_data = st.file_uploader(
    "üìÅ Upload new dataset to retrain the model (CSV or Excel)",
    type=["csv", "xlsx"],
    key="retrain_uploader"
)
 
if uploaded_new_data:
    # Load dataframe
    try:
        if uploaded_new_data.name.endswith(".csv"):
            df_new = pd.read_csv(uploaded_new_data)
        else:
            df_new = pd.read_excel(uploaded_new_data)
    except Exception as e:
        st.error(f"‚ùå Could not read the file: {e}")
        df_new = None
 
    if df_new is not None:
        st.write("‚úÖ Uploaded New Dataset Preview:")
        st.dataframe(df_new.head())
 
        target_col = "exoplanet_archive_disposition"
 
        if target_col not in df_new.columns:
            st.error(f"‚ùå Target column '{target_col}' not found!")
        else:
            # -------- First click: TRAIN --------
            if st.button("üöÄ Retrain Model", key="btn_retrain"):
                try:
                    with st.spinner("Training new model... ‚è≥"):
                        # Separate features/target
                        X_raw = df_new.drop(columns=[target_col])
                        y_raw = df_new[target_col]
 
                        # Keep numeric columns only; fill missing numerics
                        X_num = X_raw.select_dtypes(include=["float64", "int64"]).copy()
                        X_num = X_num.fillna(X_num.median(numeric_only=True))
 
                        # Encode y, scale X
                        le_new = LabelEncoder()
                        y_encoded = le_new.fit_transform(y_raw)
 
                        scaler_new = StandardScaler()
                        X_scaled = scaler_new.fit_transform(X_num)
 
                        # Split
                        X_train, X_test, y_train, y_test = train_test_split(
                            X_scaled, y_encoded, test_size=0.2, random_state=42
                        )
 
                        # Train model
                        model_new = DecisionTreeClassifier(
                            criterion="gini",
                            max_depth=10,
                            min_samples_split=5,
                            random_state=42
                        )
                        model_new.fit(X_train, y_train)
                        y_pred = model_new.predict(X_test)
 
                        # Metrics
                        acc_new = accuracy_score(y_test, y_pred)
                        report_df = pd.DataFrame(
                            classification_report(y_test, y_pred, output_dict=True)
                        ).transpose()
 
                        st.success(f"‚úÖ New Model Accuracy: **{acc_new:.4f}**")
 
                        # Compare with current model (if recorded)
                        current_version = metadata["current_version"]
                        prev_acc = metadata["versions"].get(str(current_version), {}).get("accuracy")
                        if prev_acc is not None:
                            st.info(f"üìä Current Model (v{current_version}) Accuracy: {float(prev_acc):.4f}")
                            if acc_new > prev_acc:
                                st.success("üöÄ The new model performs better!")
                            else:
                                st.warning("‚ö†Ô∏è The new model performs worse than the current version.")
 
                        st.subheader("üìà Evaluation Metrics")
                        st.dataframe(report_df)
 
                        # Candidate feature importances plot
                        try:
                            importances = pd.Series(
                                model_new.feature_importances_, index=X_num.columns
                            ).sort_values(ascending=False)
 
                            plt.figure(figsize=(7, 4))
                            sns.barplot(x=importances.head(10), y=importances.head(10).index)
                            plt.title("Top 10 Feature Importances (New Model)")
                            plt.tight_layout()
 
                            cand_plot_path = "../models/feature_importances_new.png"
                            plt.savefig(cand_plot_path)
                            st.image(cand_plot_path, width=450)
                        except Exception as e_plot:
                            cand_plot_path = None
                            st.warning(f"‚ö†Ô∏è Could not render feature importances: {e_plot}")
 
                        # Stash candidate artifacts in session_state for the Upgrade step
                        st.session_state.candidate_model = model_new
                        st.session_state.candidate_scaler = scaler_new
                        st.session_state.candidate_le = le_new
                        st.session_state.candidate_metrics = {"accuracy": float(acc_new)}
                        st.session_state.candidate_features = list(X_num.columns)
                        st.session_state.candidate_featimp_path = cand_plot_path
 
                        st.success("üß™ Candidate model trained and ready. Scroll down to upgrade.")
                except Exception as e:
                    st.error(f"‚ùå Error during retraining: {e}")
 
# -------- Second click: UPGRADE (rendered if we have a candidate) --------
if st.session_state.candidate_model is not None:
    st.markdown("---")
    st.subheader("‚¨ÜÔ∏è Upgrade to New Model Version")
 
    if st.button("üíæ Upgrade to New Model Version", key="btn_upgrade"):
        try:
            # Guardrails: make sure we have everything needed
            metrics = st.session_state.get("candidate_metrics")
            features = st.session_state.get("candidate_features")
            model_cand = st.session_state.get("candidate_model")
            scaler_cand = st.session_state.get("candidate_scaler")
            le_cand = st.session_state.get("candidate_le")
 
            if not isinstance(metrics, dict) or "accuracy" not in metrics:
                st.error("No candidate metrics found. Please retrain first.")
                st.stop()
            if model_cand is None or scaler_cand is None or le_cand is None or not features:
                st.error("Incomplete candidate artifacts. Please retrain first.")
                st.stop()
 
            # Compute new version
            current_version = metadata["current_version"]
            new_version = current_version + 1
 
            # Persist versioned artifacts
            joblib.dump(model_cand, f"../models/trained_model_v{new_version}.pkl")
            joblib.dump(scaler_cand, f"../models/scaler_v{new_version}.pkl")
            joblib.dump(le_cand, f"../models/label_encoder_v{new_version}.pkl")
 
            # Save feature list for the new version and also update the unversioned pointer
            joblib.dump(features, f"../models/feature_list_v{new_version}.pkl")
            joblib.dump(features, "../models/feature_list.pkl")
 
            # Save/rename feature importance image to match versioned naming
            versioned_plot = f"../models/feature_importances_v{new_version}.png"
            cand_plot_path = st.session_state.get("candidate_featimp_path")
            if cand_plot_path and os.path.exists(cand_plot_path):
                # move/replace the candidate plot into versioned filename
                os.replace(cand_plot_path, versioned_plot)
 
            # Update metadata.json
            metadata["current_version"] = new_version
            metadata["versions"][str(new_version)] = {
                "accuracy": float(metrics["accuracy"]),
                "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            with open(metadata_path, "w") as f:
                json.dump(metadata, f, indent=4)
 
            # Clear candidate artifacts
            for k in [
                "candidate_model", "candidate_scaler", "candidate_le",
                "candidate_metrics", "candidate_features", "candidate_featimp_path"
            ]:
                st.session_state[k] = None
 
            st.success(f"üéâ Model upgraded to version {new_version} successfully!")
            st.info("üîÅ Rerun/refresh the app to load the new current version at the top.")
 
            # Optional: trigger an immediate rerun to refresh header/version
            # st.rerun()
 
        except Exception as e:
            st.error(f"‚ùå Error while upgrading model: {e}")
 
 
st.write("For detailed information about each feature, please check this link:")
 
st.markdown(
    """
    <a href="https://exoplanetarchive.ipac.caltech.edu/docs/API_kepcandidate_columns.html#pdisposition" target="_blank">
        <button style="padding:10px; font-size:16px;">Open Exoplanet Archive</button>
    </a>
    """,
    unsafe_allow_html=True
)